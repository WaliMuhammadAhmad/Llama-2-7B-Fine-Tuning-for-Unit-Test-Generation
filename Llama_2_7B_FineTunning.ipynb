{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"103bfb0b86274d66aa53e7a4c2e1d9a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47eb0be427c94e8392716c62baabdbab","max":7440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abeec778ffef48c1ac3f0267f26d5e07","value":7440}},"172f6d2ee3cb4dee85834b35aa2b09f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25bd256297e24e29867e61cb979e7772":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3786b9aee3a345098eb60ca556aa2a2d","max":7440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da1a2a37fd8244f997abad1b7b15eaa1","value":4471}},"2f9c551803b24af2b3eca0589b6b564f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3786b9aee3a345098eb60ca556aa2a2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47eb0be427c94e8392716c62baabdbab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5593ac57a874458dbdd2fcd988ecaeb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8912b7dd006a4a49953c13bcb171b33a","placeholder":"​","style":"IPY_MODEL_e97c816e6f914ec0af7cfcf588b6a5f9","value":" 4471/7440 [02:00&lt;01:05, 45.59files/s]"}},"5aaaea676e7c43fa9011f34280671e38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_172f6d2ee3cb4dee85834b35aa2b09f5","placeholder":"​","style":"IPY_MODEL_7d84b53a0414491aa2df96c7fbd67ed2","value":"Downloading data:  60%"}},"7d84b53a0414491aa2df96c7fbd67ed2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"860360984c52459984bf562fd983f871":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8912b7dd006a4a49953c13bcb171b33a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f927f24266b4465835ed81568a7e9a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9994deff07a341a0aa864a66b7a5ba4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5aaaea676e7c43fa9011f34280671e38","IPY_MODEL_25bd256297e24e29867e61cb979e7772","IPY_MODEL_5593ac57a874458dbdd2fcd988ecaeb6"],"layout":"IPY_MODEL_860360984c52459984bf562fd983f871"}},"a50f727ba841446d992c88278af41754":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8df90fb8d5c4d2399c1e5e147004b6a","placeholder":"​","style":"IPY_MODEL_8f927f24266b4465835ed81568a7e9a1","value":" 7440/7440 [00:00&lt;00:00,  7.53it/s]"}},"abeec778ffef48c1ac3f0267f26d5e07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b485a6acb14e4bae81b5a0ba077270e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8baa6bae3ce4af0a535d631ef5907df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6135cece18149b19bf500d2d55a8ac7","IPY_MODEL_103bfb0b86274d66aa53e7a4c2e1d9a7","IPY_MODEL_a50f727ba841446d992c88278af41754"],"layout":"IPY_MODEL_2f9c551803b24af2b3eca0589b6b564f"}},"c8df90fb8d5c4d2399c1e5e147004b6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6135cece18149b19bf500d2d55a8ac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b485a6acb14e4bae81b5a0ba077270e3","placeholder":"​","style":"IPY_MODEL_f6bc2649da6d44dfa335fbf7944ac385","value":"Resolving data files: 100%"}},"da1a2a37fd8244f997abad1b7b15eaa1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e97c816e6f914ec0af7cfcf588b6a5f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6bc2649da6d44dfa335fbf7944ac385":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/WaliMuhammadAhmad/ReviewsSentimentalAnalysis/blob/main/Llama_2_7B_FineTunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFiYvR61Zk3m","outputId":"0cde18c2-753c-4d5b-a3dc-0c2cd41b7b4f","execution":{"iopub.status.busy":"2024-05-20T16:45:48.496331Z","iopub.execute_input":"2024-05-20T16:45:48.496684Z","iopub.status.idle":"2024-05-20T16:46:06.460326Z","shell.execute_reply.started":"2024-05-20T16:45:48.496651Z","shell.execute_reply":"2024-05-20T16:46:06.459111Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !pip install transformer==4.33.1","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:06.462753Z","iopub.execute_input":"2024-05-20T16:46:06.463044Z","iopub.status.idle":"2024-05-20T16:46:06.467345Z","shell.execute_reply.started":"2024-05-20T16:46:06.463018Z","shell.execute_reply":"2024-05-20T16:46:06.466515Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install transformers==4.31.0","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:06.468301Z","iopub.execute_input":"2024-05-20T16:46:06.468612Z","iopub.status.idle":"2024-05-20T16:46:30.278639Z","shell.execute_reply.started":"2024-05-20T16:46:06.468590Z","shell.execute_reply":"2024-05-20T16:46:30.277416Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2024.2.2)\nDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"ul128W0XZ00t","execution":{"iopub.status.busy":"2024-05-20T16:46:30.281294Z","iopub.execute_input":"2024-05-20T16:46:30.281608Z","iopub.status.idle":"2024-05-20T16:46:52.111970Z","shell.execute_reply.started":"2024-05-20T16:46:30.281580Z","shell.execute_reply":"2024-05-20T16:46:52.111229Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-05-20 16:46:39.258423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-20 16:46:39.258551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-20 16:46:39.434690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-finetune\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n# i recommend use \n#lora_r = 16\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n# comment the dropout if lora_r = 16\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"/kaggle/working/\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True # must\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 2\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 2\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"id":"UhDBZDcxZ-Th","execution":{"iopub.status.busy":"2024-05-20T16:46:52.113240Z","iopub.execute_input":"2024-05-20T16:46:52.113509Z","iopub.status.idle":"2024-05-20T16:46:52.123344Z","shell.execute_reply.started":"2024-05-20T16:46:52.113486Z","shell.execute_reply":"2024-05-20T16:46:52.122465Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"id":"Ylarxyue0ieg","execution":{"iopub.status.busy":"2024-05-20T16:46:52.124531Z","iopub.execute_input":"2024-05-20T16:46:52.124790Z","iopub.status.idle":"2024-05-20T16:46:53.108323Z","shell.execute_reply.started":"2024-05-20T16:46:52.124768Z","shell.execute_reply":"2024-05-20T16:46:53.107063Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:53.109763Z","iopub.execute_input":"2024-05-20T16:46:53.110093Z","iopub.status.idle":"2024-05-20T16:46:53.117272Z","shell.execute_reply.started":"2024-05-20T16:46:53.110065Z","shell.execute_reply":"2024-05-20T16:46:53.116473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# The instruction dataset to use\ndataset = \"Shafeen/test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:53.118419Z","iopub.execute_input":"2024-05-20T16:46:53.118769Z","iopub.status.idle":"2024-05-20T16:46:53.131269Z","shell.execute_reply.started":"2024-05-20T16:46:53.118743Z","shell.execute_reply":"2024-05-20T16:46:53.130424Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:53.132396Z","iopub.execute_input":"2024-05-20T16:46:53.132967Z","iopub.status.idle":"2024-05-20T16:46:54.206147Z","shell.execute_reply.started":"2024-05-20T16:46:53.132938Z","shell.execute_reply":"2024-05-20T16:46:54.205014Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Mon May 20 16:46:54 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"### Load dataset (you can process it here)\n# !huggingface-cli login\n!huggingface-cli login --token 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\n\n\n# dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.select(range(3000))\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":995,"referenced_widgets":["b8baa6bae3ce4af0a535d631ef5907df","d6135cece18149b19bf500d2d55a8ac7","103bfb0b86274d66aa53e7a4c2e1d9a7","a50f727ba841446d992c88278af41754","2f9c551803b24af2b3eca0589b6b564f","b485a6acb14e4bae81b5a0ba077270e3","f6bc2649da6d44dfa335fbf7944ac385","47eb0be427c94e8392716c62baabdbab","abeec778ffef48c1ac3f0267f26d5e07","c8df90fb8d5c4d2399c1e5e147004b6a","8f927f24266b4465835ed81568a7e9a1","9994deff07a341a0aa864a66b7a5ba4c","5aaaea676e7c43fa9011f34280671e38","25bd256297e24e29867e61cb979e7772","5593ac57a874458dbdd2fcd988ecaeb6","860360984c52459984bf562fd983f871","172f6d2ee3cb4dee85834b35aa2b09f5","7d84b53a0414491aa2df96c7fbd67ed2","3786b9aee3a345098eb60ca556aa2a2d","da1a2a37fd8244f997abad1b7b15eaa1","8912b7dd006a4a49953c13bcb171b33a","e97c816e6f914ec0af7cfcf588b6a5f9"]},"id":"AyIswqF7vzbn","outputId":"0e5e0122-1fc1-4282-f3c0-bdd5c183c2ac","execution":{"iopub.status.busy":"2024-05-20T16:46:54.209551Z","iopub.execute_input":"2024-05-20T16:46:54.209846Z","iopub.status.idle":"2024-05-20T16:46:56.614130Z","shell.execute_reply.started":"2024-05-20T16:46:54.209820Z","shell.execute_reply":"2024-05-20T16:46:56.612913Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface-cli login --token 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\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dataset = load_dataset(dataset_name, split=\"train\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model with QLoRA configuration\u001b[39;00m\n\u001b[1;32m      9\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, bnb_4bit_compute_dtype)\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'select'"],"ename":"AttributeError","evalue":"'str' object has no attribute 'select'","output_type":"error"}]},{"cell_type":"code","source":"# trainer.model.save_pretrained(\"Llama-2-7b-chat-finetune\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:46:56.614925Z","iopub.status.idle":"2024-05-20T16:46:56.615316Z","shell.execute_reply.started":"2024-05-20T16:46:56.615117Z","shell.execute_reply":"2024-05-20T16:46:56.615152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"id":"-DhigjwyifA1","execution":{"iopub.status.busy":"2024-05-20T16:46:56.617009Z","iopub.status.idle":"2024-05-20T16:46:56.617381Z","shell.execute_reply.started":"2024-05-20T16:46:56.617219Z","shell.execute_reply":"2024-05-20T16:46:56.617233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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\n\nmodel.push_to_hub(\"phoenixLisha/Llama-2-7b-chat-finetune\", check_pr=True)\n\ntokenizer.push_to_hub(\"phoenixLisha/Llama-2-7b-chat-finetune\",check_pr=True)\n","metadata":{"id":"cFqoROcTijcx","execution":{"iopub.status.busy":"2024-05-20T16:46:56.619086Z","iopub.status.idle":"2024-05-20T16:46:56.619409Z","shell.execute_reply.started":"2024-05-20T16:46:56.619260Z","shell.execute_reply":"2024-05-20T16:46:56.619272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"id":"QNxArZKXCBoq","execution":{"iopub.status.busy":"2024-05-20T16:46:56.620469Z","iopub.status.idle":"2024-05-20T16:46:56.620763Z","shell.execute_reply.started":"2024-05-20T16:46:56.620614Z","shell.execute_reply":"2024-05-20T16:46:56.620626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"SortUtils { public static <T> List<T> sort(Collection<T> objects, Visitor<T> visitor) throws IllegalNodeConfigException, CircularReferencesException { if (objects.size() == 0) { return Collections.emptyList(); } final Map<String, Node<T>> nodes = new LinkedHashMap<String, Node<T>>(); for (T obj : objects) { String name = visitor.getName(obj); Node<T> node = new Node<T>(name, obj, visitor.afterOthers(obj), visitor.beforeOthers(obj), visitor.getAfterNames(obj), visitor.getBeforeNames(obj)); if (node.beforeOthers && node.afterOthers) { throw new IllegalNodeConfigException(name, \\\"Before others and after others could not be configured at the sametime \\\"); } nodes.put(name, node); } for (Node<T> node : nodes.values()) { if (node.after.contains(node.name)) { throw new IllegalNodeConfigException(node.name, \\\"The fragment could not be configured to after itself\\\"); } for (String afterNodeName : node.after) { Node<T> afterNode = nodes.get(afterNodeName); if (afterNode != null) { node.dependOns.add(afterNode); } } if (node.before.contains(node.name)) { throw new IllegalNodeConfigException(node.name, \\\"The fragment could not be configured to before itself\\\"); } for (String beforeNodeName : node.before) { Node<T> beforeNode = nodes.get(beforeNodeName); if (beforeNode != null) { beforeNode.dependOns.add(node); } } } boolean circuitFounded = false; for (Node<T> node : nodes.values()) { Set<Node<T>> visitedNodes = new HashSet<Node<T>>(); if (!normalizeNodeReferences(node, node, visitedNodes)) { circuitFounded = true; break; } node.dependOns.addAll(visitedNodes); } if (circuitFounded) { Set<Circuit<T>> circuits = new LinkedHashSet<Circuit<T>>(); for (Node<T> node : nodes.values()) { findCircuits(circuits, node, new java.util.Stack<Node<T>>()); } ArrayList<Circuit<T>> list = new ArrayList<Circuit<T>>(circuits); Collections.sort(list); List<List> all = new ArrayList<List>(); for (Circuit circuit : list) { all.add(unwrap(circuit.nodes)); } throw new CircularReferencesException(all); } Node<T> rootNode = new Node<T>(); rootNode.previous = rootNode; rootNode.next = nodes.values().iterator().next(); for (Node<T> node : nodes.values()) { node.previous = rootNode.previous; rootNode.previous.next = node; node.next = rootNode; rootNode.previous = node; } Node<T> lastBeforeNode = rootNode; for (Node<T> node : nodes.values()) { if (node.beforeOthers) { moveAfter(node, lastBeforeNode); lastBeforeNode = node; } else if (node.afterOthers) { moveBefore(node, rootNode); } } for (Node<T> node : nodes.values()) { for (Node<T> reference : node.dependOns) { swap(node, reference, rootNode); } } List<T> sortedList = new ArrayList<T>(nodes.size()); Node<T> currentNode = rootNode.next; while (currentNode != rootNode) { sortedList.add(currentNode.object); currentNode = currentNode.next; } return sortedList; }  static List<T> sort(Collection<T> objects, Visitor<T> visitor);  }\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"id":"Q7znfpTWJVLj","execution":{"iopub.status.busy":"2024-05-20T16:46:56.622769Z","iopub.status.idle":"2024-05-20T16:46:56.623258Z","shell.execute_reply.started":"2024-05-20T16:46:56.622996Z","shell.execute_reply":"2024-05-20T16:46:56.623016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}